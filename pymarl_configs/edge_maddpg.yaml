alg: maddpg
runner: episode
env: edge_marl
env_args:
  n_nodes: 10
  node_cpu_capacity: [1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 3.0]
  task_arrival:
    mode: cyclic
    rate_range: [5, 15]        # tasks per second
    cycle_seconds: 20.0
  task_workload_range: [1.0, 10.0]
  task_data_range: [0.5, 5.0]
  task_deadline_range_ms: [50, 500]
  episode_length: 1000
  reward_weights: {alpha: 0.5, beta: 0.3, gamma: 0.2}

agent:
  type: maddpg
  hidden_dim: 64
  actor_lr: 0.0003
  critic_lr: 0.0003
  gamma: 0.99
  tau: 0.01
  batch_size: 128
  buffer_size: 100000
  target_update_interval: 100
  actor_layers: [64, 64]
  critic_layers: [64, 64]
  action_format:
    placement_logits: true
    resource_ratio: true

train:
  total_timesteps: 5000000
  evaluation_interval: 50000
  evaluation_episodes: 100
  num_seeds: [0, 1, 2, 3, 4]

logging:
  metrics: ["avg_latency_ms", "p99_latency_ms", "avg_energy_J", "throughput_tps", "load_balance_jain"]

