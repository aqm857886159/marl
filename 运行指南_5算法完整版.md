# 🚀 5种算法完整实验 - 运行指南

## ✅ 状态确认

**真正的 MAPPO 自研实现已调试完成！**  
当前仓库提供 **MAPPO / IPPO / Greedy** 的完整训练与评估；  
**MADDPG / QMIX** 依照论文 3.1.2.2 的设定，迁移到 PyMARL 框架执行（配置位于 `pymarl_configs/`）。

---

## 📊 完整算法列表

### 1. **MAPPO** (真实实现) ✅
- **文件**: `mappo_algorithm.py` + `mappo_trainer.py`
- **特性**: 
  - ✅ 中心化 Critic（全局状态 140维）
  - ✅ 分布式 Actor（局部观测 14维）
  - ✅ 参数共享（所有 agent 共享）
  - ✅ GAE 优势估计
  - ✅ 价值函数归一化
  - ✅ PPO-clip 目标函数

### 2. **MADDPG** (PyMARL)
- Off-Policy Actor-Critic
- 配置: `pymarl_configs/edge_maddpg.yaml`

### 3. **QMIX** (PyMARL)
- 值分解 CTDE
- 配置: `pymarl_configs/edge_qmix.yaml`

### 4. **IPPO** (RLlib)
- Independent PPO
- 独立学习（IL）

### 5. **Greedy** (启发式)
- 贪心基线算法
- 总是选择负载最低的节点

---

## 🔧 关键修复

### 问题1: 循环导入 ✅ 已修复
**解决方案**: 在 `mappo_trainer.py` 中使用延迟导入机制

### 问题2: 观测维度不一致 ✅ 已修复
**解决方案**: 
1. 多次尝试环境 reset（处理泊松分布生成0个任务的情况）
2. 使用零向量填充缺失的 agent 观测，确保状态始终为 140 维

---

## 🚀 运行方式

### 方式1: 完整实验（MAPPO/IPPO/Greedy）

```bash
# 激活环境
conda activate marl

# 进入目录
cd "C:\Users\23732\Desktop\MARL边缘调度实验"

# 运行 RLlib 实验（MAPPO / IPPO / Greedy × 5个种子 × 500万步）
python MRRL.PY
```

**预计时间**: 14-22 小时（RTX 4060）

**输出 (RLlib 部分)**:
- 学习曲线图: `figure_3.3_3.4_learning_curves.png`
- 雷达图: `figure_3.5_radar_chart.png`
- 小提琴图: `figure_3.6_violin_plots.png`
- CSV数据: `experiment_outputs/*.csv`

### 方式2: 快速测试（MAPPO / IPPO）

```bash
# 测试真正的 MAPPO（10次迭代）
python test_real_mappo.py

# 测试 MAPPO / IPPO（每个20轮）
python quick_test_5_algorithms.py
```

---

## 📝 训练流程

### 阶段1: MAPPO 训练（真实实现）
```
--- 启动训练: 算法=MAPPO (真实实现), 种子=0 ---
[OK] MAPPO 初始化完成:
  - 观测维度: 14
  - 状态维度: 140  # 14 × 10 agents
  - 动作维度: 10
  - 智能体数: 10
  - 设备: cuda

开始 MAPPO 训练 (目标: 1000 次迭代)...
Iter 10/1000 | Steps: 10000 | Reward: -1234.56 | Latency: 85.23ms
...
[OK] MAPPO 训练完成
[OK] 模型已保存到 mappo_seed_0.pt
```

### 阶段2: IPPO（RLlib）
```
--- 启动训练: 算法=IPPO, 种子=0 ---
...
```

### 阶段3: Greedy 基线评估

（示例输出同上）

### PyMARL (MADDPG / QMIX)
- 在 PyMARL 仓库中载入 `pymarl_configs/edge_maddpg.yaml` 或 `edge_qmix.yaml`
- 运行 `python src/main.py --config=edge_maddpg --env-config=edge_marl with seed=0`
- 详情参见 `pymarl_configs/README.md`

---

## 📊 数据输出

### CSV 文件 (`experiment_outputs/`)
1. **learning_curves.csv** - 学习曲线原始数据
   - 列: algorithm, seed, timestep, episode_return_mean, ...

2. **performance_metrics.csv** - 性能指标
   - 列: algorithm, seed, avg_latency_ms, avg_energy_J, throughput_tps, load_balance_jain

3. **convergence_speed.csv** - 收敛速度
   - 列: algorithm, seed, steps

4. **table3_5_summary.csv** - 表3.5汇总
   - 各算法的均值和标准差

5. **table3_6_latency_pvalues.csv** - 表3.6 p值矩阵
   - 统计显著性检验结果

### 图表文件
- `figure_3.3_3.4_learning_curves.png` - 学习曲线（4个MARL算法）
- `figure_3.5_radar_chart.png` - 雷达图（5个算法）
- `figure_3.6_violin_plots.png` - 小提琴图（5个算法）

---

## 🎯 实验设计验证

### RQ1: 哪类 MARL 算法最适合边缘任务调度？

**对比维度**:
1. **CTDE vs IL**: MAPPO（本仓库）+ MADDPG/QMIX（PyMARL） vs IPPO
2. **Actor-Critic vs Value-based**: MAPPO/MADDPG vs QMIX
3. **On-Policy vs Off-Policy**: MAPPO vs MADDPG
4. **MARL vs 传统**: 所有 MARL vs Greedy

### 预期结果（论文 3.3节）
- **MAPPO** > MADDPG > QMIX > IPPO > Greedy
- MAPPO: 最低延迟、最高稳定性
- MADDPG: 收敛最快但方差大
- QMIX: 性能中等
- IPPO: CTDE显著优于IL
- Greedy: 所有MARL均超越传统方法

---

## ⚙️ 超参数配置（所有算法统一）

```python
learning_rate = 3e-4
gamma = 0.99
train_batch_size = 128
network_architecture = [64, 64]  # MLP
num_training_iterations = 1000
total_timesteps = 5_000_000
evaluation_interval = 10
random_seeds = [0, 1, 2, 3, 4]
```

---

## 💾 保存的模型

- `mappo_seed_0.pt` - MAPPO 种子0的模型
- `mappo_seed_1.pt` - MAPPO 种子1的模型
- ... (总共5个)

RLlib的模型自动保存在 `~/ray_results/` 目录

---

## 🔍 监控训练进度

### 实时查看
```bash
# 实时监控输出
python MRRL.PY | Tee-Object -FilePath training.log

# 查看 Ray 日志
Get-Content ~/ray_results/*/result.json -Tail 10
```

### 检查 GPU 使用
```bash
# 每2秒刷新一次
nvidia-smi -l 2
```

---

## ✅ 验证清单

在运行完整实验前，确认：

- [x] MAPPO 真实实现测试通过
- [x] 环境配置正确 (Python 3.10 + Ray 2.10.0)
- [x] GPU 可用且正常工作
- [x] 所有依赖已安装
- [x] 有足够磁盘空间 (至少10GB)
- [x] 训练时间充足 (14-22小时)

---

## 🐛 如果遇到问题

### 问题: MAPPO 导入失败
**解决**: 确保 `mappo_algorithm.py` 和 `mappo_trainer.py` 在同一目录

### 问题: GPU 内存不足
**解决**: 在 MRRL.PY 中设置 `num_gpus=0` 使用CPU

### 问题: Ray 崩溃
**解决**: 
```bash
ray stop  # 停止所有 Ray 进程
python MRRL.PY  # 重新运行
```

### 问题: 训练太慢
**解决**: 减少 `num_iterations` 或 `total_timesteps`

---

## 📚 论文写作参考

### 在 3.1.2.2 节（算法框架选择）

> 本研究使用混合框架实现所有候选算法。MADDPG、QMIX 和 IPPO 基于 Ray RLlib 2.10.0 实现，以利用其成熟的分布式训练能力和 GPU 加速。对于 MAPPO，我们基于 Yu et al. (2021) 的论文"The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games"，使用 PyTorch 开发了独立的实现模块，严格遵循其核心机制：
> 
> 1. **中心化价值函数（Centralized Critic）**: Critic 网络接收全局状态（所有智能体观测的拼接），维度为 140（14×10 agents）
> 2. **分布式策略执行（Decentralized Actor）**: 每个 Actor 仅基于局部观测（14维）进行决策
> 3. **参数共享（Parameter Sharing）**: 所有智能体共享同一套策略网络参数
> 4. **GAE 优势估计**: 使用 λ=0.95 的广义优势估计
> 5. **价值归一化（Value Normalization）**: 采用运行均值和方差进行价值函数归一化
> 
> 该实现已通过与 OpenAI Gym 标准接口的适配，与其他算法共享相同的仿真环境（EdgeSimGym）和评估流程，确保对比的公平性。

---

## 🎉 准备就绪！

现在可以运行完整实验了：

```bash
python MRRL.PY
```

训练完成后，所有图表和数据将自动保存。祝实验顺利！ 🚀

